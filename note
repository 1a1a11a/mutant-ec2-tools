TODOs
-----
Low priority. When a cluter is cleaned by ClusterCleaner, re-launch the job
(make the same job request again). Doesn't the reappearing of SQS message
already do this?  Double check!

Low prirotiy. Admission control. Launch a new cluster when the number of nodes
in each DC is less than 12. InstMonitor has the info. Not sure if I will need
it.


Notes
-----
On MacOS, ImportError: No module named boto3
- Can't figure it out. Might be from the pip upgrade. Move to mt-s7.

Update installed pip packages on Mac OS
	root# time pip freeze --local | grep -v '^\-e' | cut -d = -f 1  | xargs -n1 pip install -U --ignore-installed six
	- http://stackoverflow.com/questions/2720014/upgrading-all-packages-with-pip
	- Takes about 5 mins.

Installing boto3 on El Capitan, which doesn't allow modifying the OS manged six
module.
	$ sudo -H pip install boto3 --upgrade --ignore-installed six
	- https://github.com/pypa/pip/issues/3165

Clean up logs before making a new AMI. You may want to put something in /var/log
	$ sudo rm /var/log/cloud-init*

$ sudo -H pip install boto3 PyYAML


Done
----
For Mutant, ClusterMonitor can be modified as JobMonitor. Each job consists of
servers and clients (most probably a single client will do).

When a cluster doesn't start in 6 mins, kill the instances. Spot requests don't
need to be cancelled. They are one-time requests.  With the async cluster
start, this is achieved for nothing.

Need a log file, mapping job_id and parameters.
- Job request
	- Received
	- Served
	- Failed
- Job completion
	- Received
	- Served
	- Failed: I haven't seen it fails.

When a cluster doesn't finish within 1 hr, kill it. It happens with a 11 node
cluster. One example is that one of the nodes didn't clone code from github.com
	sudo: unable to resolve host ip-172-31-17-235
	Cloning into '/home/ubuntu/work/ec2-tools'...
	fatal: unable to access 'https://github.com/hobinyoon/ec2-tools.git/': Could not resolve host: github.com

When a job controller sees an acorn-server cluster with less than 11 nodes for
6 mins, terminate the cluster. job-controller will get the job request in 1
hour after the visibility timeout.  This happens quite often, due to the spot
price increase. Setting the spot request price really high can be a good idea
too.

term-inst.py
- Kill all instances without job_id

Pick an AZ with the lowest last-day max pricing.
